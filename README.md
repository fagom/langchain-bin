# Projects

- Ollama Chat
  - Run `poetry install` to install all dependencies
  - Start docker compose to run ollama service in the background
  - Run `poetry run python src/ollama_chat/app.py` to invoke chat models and get response from the LLM
